{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUIcYK5rCEAw",
        "outputId": "6ff23b25-3be3-4ef6-ddbf-ba0c80af608f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3FRBd7XBL0i",
        "outputId": "0e880a91-f23e-4297-ac3d-24bb7beabb07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "BASE_MODEL = \"facebook/xglm-564M\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
        "\n",
        "# Padding fix for GPT-style models\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pth_path = \"/content/drive/MyDrive/I5_WR_Project/xglm_khmer_word.pth\"\n",
        "\n",
        "state_dict = torch.load(pth_path, map_location=device)\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"✅ XGLM model loaded from .pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5ZVULK_BNOx",
        "outputId": "a4f00d11-df39-48fb-ef07-da821138e03e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ XGLM model loaded from .pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"ខ្ញុំ ចង់ ទៅ\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=10,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.8\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UY7suEXCoGh",
        "outputId": "e7cb6b79-6229-4cce-ea5a-d2217a85897f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ខ្ញុំ ចង់ ទៅ ដល់ សួរ គេ ថា លោក មាន ការ អ្វី បាន\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def top10_next_tokens(prompt, top_k=10):\n",
        "    model.eval()\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # logits shape: [batch, seq_len, vocab]\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # take last token logits\n",
        "    next_token_logits = logits[0, -1]\n",
        "\n",
        "    probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    top_probs, top_ids = torch.topk(probs, top_k)\n",
        "\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for rank, (prob, token_id) in enumerate(zip(top_probs, top_ids), start=1):\n",
        "        token = tokenizer.decode([token_id.item()])\n",
        "        token = token.replace(\"▁\", \" \")  # SentencePiece fix\n",
        "        print(f\"{rank:02d}. '{token}'  →  {prob.item():.4f}\")\n",
        "\n",
        "top10_next_tokens(\"ខ្ញុំ ចង់ ទៅ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lID2M1g5CTmk",
        "outputId": "86ca3808-e1fc-4cee-805e-2fb9250c90e4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: ខ្ញុំ ចង់ ទៅ\n",
            "----------------------------------------\n",
            "01. 'ដល់'  →  0.5236\n",
            "02. 'ទៀត'  →  0.2419\n",
            "03. ''  →  0.0821\n",
            "04. 'ដ'  →  0.0284\n",
            "05. 'ឃើញ'  →  0.0258\n",
            "06. 'លោក'  →  0.0155\n",
            "07. 'ក្នុង'  →  0.0124\n",
            "08. 'ឲ្យ'  →  0.0086\n",
            "09. 'នៅ'  →  0.0086\n",
            "10. 'រក'  →  0.0069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def autosuggest(prompt, top_k=5):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits[0, -1]\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    top_probs, top_ids = torch.topk(probs, top_k)\n",
        "\n",
        "    suggestions = []\n",
        "    for p, idx in zip(top_probs, top_ids):\n",
        "        token = tokenizer.decode([idx.item()])\n",
        "        token = token.replace(\"▁\", \" \").strip()\n",
        "        suggestions.append((token, p.item()))\n",
        "\n",
        "    return suggestions\n"
      ],
      "metadata": {
        "id": "JvqfMj0WIfGb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Khmer Keyboard Auto-Suggestion Demo ===\")\n",
        "print(\"Type a sentence, press ENTER (type 'exit' to quit)\\n\")\n",
        "\n",
        "while True:\n",
        "    prompt = input(\"Input: \").strip()\n",
        "    if prompt.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    suggestions = autosuggest(prompt, top_k=5)\n",
        "\n",
        "    print(\"Suggestions:\")\n",
        "    for i, (word, prob) in enumerate(suggestions, 1):\n",
        "        print(f\" {i}. {word}  ({prob:.3f})\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2sJlBPbIRkS",
        "outputId": "73fe5712-48af-4cb0-e007-332443c674cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Khmer Keyboard Auto-Suggestion Demo ===\n",
            "Type a sentence, press ENTER (type 'exit' to quit)\n",
            "\n",
            "Input: ចង់\n",
            "Suggestions:\n",
            " 1. បញ្ជាក់  (0.212)\n",
            " 2. ច  (0.166)\n",
            " 3.   (0.153)\n",
            " 4. ប្រើ  (0.082)\n",
            " 5. សេ  (0.037)\n",
            "\n",
            "Input: exit\n"
          ]
        }
      ]
    }
  ]
}